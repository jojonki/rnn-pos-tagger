{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Dropout, RepeatVector\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import Model\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/jonki/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "sents = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = collections.Counter()\n",
    "data = []\n",
    "for sent in sents:\n",
    "    sentence, poss = [], []\n",
    "    for word, pos in sent:\n",
    "        if pos == '-NONE-': continue\n",
    "        sentence.append(word)\n",
    "        word_freq[word] += 1\n",
    "        poss.append(pos)\n",
    "    data.append((sentence, poss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_vocab 5002\n",
      "sentence maxlen 249\n",
      "pos_vocab 46\n"
     ]
    }
   ],
   "source": [
    "max_raw_vocab = 5000\n",
    "raw_vocab_size = min(max_raw_vocab, len(word_freq)) + 2\n",
    "raw_w2i = {w[0]:i for i, w in enumerate(word_freq.most_common(max_raw_vocab), 2)}\n",
    "PAD = '<PADDING>'\n",
    "UNK = '<UNK>'\n",
    "raw_w2i[PAD], raw_w2i[UNK] = 0, 1\n",
    "raw_i2w = {v:k for k, v in raw_w2i.items()}\n",
    "print('raw_vocab', len(raw_w2i))\n",
    "\n",
    "sentence_maxlen = max([len(sent) for sent, pos in data])\n",
    "print('sentence maxlen', sentence_maxlen)\n",
    "\n",
    "pos_vocab = set()\n",
    "for sentence, poss in data:\n",
    "#     raw_vocab |= set(sentence)\n",
    "    pos_vocab |= set(poss)\n",
    "\n",
    "pos_w2i = {p:i for i, p in enumerate(pos_vocab, 1)}\n",
    "pos_w2i[PAD] = 0\n",
    "pos_i2w = {v:k for k, v in enumerate(pos_w2i.items())}\n",
    "pos_vocab_size = len(pos_w2i)\n",
    "# print('raw_vocab', len(raw_vocab))\n",
    "print('pos_vocab', pos_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N 3914\n",
      "(['The', 'latest', '10-year', 'notes', 'were', 'quoted', 'at', '100', '22\\\\/32', 'to', 'yield', '7.88', '%', 'compared', 'with', '100', '16\\\\/32', 'to', 'yield', '7.90', '%', '.'], ['DT', 'JJS', 'JJ', 'NNS', 'VBD', 'VBN', 'IN', 'CD', 'CD', 'TO', 'VB', 'CD', 'NN', 'VBN', 'IN', 'CD', 'CD', 'TO', 'VB', 'CD', 'NN', '.'])\n",
      "[  14  482 2347  324   50 1670   23  247 3886    6  187 2865   21  305   24\n",
      "  247    1    6  187 2501   21    4    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# vectorize\n",
    "N = len(data)\n",
    "random.shuffle(data)\n",
    "X = np.zeros((N, sentence_maxlen), dtype=np.uint32)\n",
    "Y = np.zeros((N, sentence_maxlen, pos_vocab_size))\n",
    "print('N', N)\n",
    "for i, (sentence, poss) in enumerate(data):\n",
    "    for t, (word, pos) in enumerate(zip(sentence, poss)):\n",
    "        if word in raw_w2i:\n",
    "            X[i, t] = raw_w2i[word]\n",
    "        else:\n",
    "            X[i, t] = raw_w2i[UNK]\n",
    "        Y[i, t, pos_w2i[pos]] = 1 # one-hot\n",
    "\n",
    "print(data[0])\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 249)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 249, 128)          640256    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 249, 64)           24768     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 249, 46)           2990      \n",
      "=================================================================\n",
      "Total params: 705,070\n",
      "Trainable params: 705,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "embd_size = 128\n",
    "latendt_size = 64\n",
    "sentence_input = Input(shape=(sentence_maxlen,))\n",
    "embd_sentence = Embedding(input_dim=raw_vocab_size, output_dim=embd_size)(sentence_input)\n",
    "rnn_out = GRU(latendt_size)(embd_sentence)\n",
    "repeated_in = RepeatVector(sentence_maxlen)(rnn_out)\n",
    "rnn_out2 = GRU(latendt_size, return_sequences=True)(repeated_in)\n",
    "pos_out = TimeDistributed(Dense(pos_vocab_size, activation='softmax'))(rnn_out2)\n",
    "# pos_out = Dense(len(pos_vocab), activation='softmax')(rnn_out2)\n",
    "model = Model(sentence_input, pos_out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.core import Activation, Dense, Dropout, RepeatVector, SpatialDropout1D\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(raw_vocab_size, embd_size,input_length=sentence_maxlen))\n",
    "# # model.add(SpatialDropout1D(Dropout(0.2)))\n",
    "# model.add(GRU(latendt_size, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(RepeatVector(sentence_maxlen))\n",
    "# model.add(GRU(latendt_size, return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(len(pos_vocab))))\n",
    "# model.add(Activation(\"softmax\"))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/30\n",
      "3131/3131 [==============================] - 14s 4ms/step - loss: 0.3183 - acc: 0.0118 - val_loss: 0.2815 - val_acc: 0.0139\n",
      "Epoch 2/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2901 - acc: 0.0145 - val_loss: 0.2791 - val_acc: 0.0140\n",
      "Epoch 3/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2886 - acc: 0.0146 - val_loss: 0.2782 - val_acc: 0.0140\n",
      "Epoch 4/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2878 - acc: 0.0146 - val_loss: 0.2775 - val_acc: 0.0140\n",
      "Epoch 5/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2871 - acc: 0.0146 - val_loss: 0.2771 - val_acc: 0.0140\n",
      "Epoch 6/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2869 - acc: 0.0146 - val_loss: 0.2771 - val_acc: 0.0140\n",
      "Epoch 7/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2868 - acc: 0.0146 - val_loss: 0.2769 - val_acc: 0.0140\n",
      "Epoch 8/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2866 - acc: 0.0146 - val_loss: 0.2768 - val_acc: 0.0140\n",
      "Epoch 9/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2865 - acc: 0.0145 - val_loss: 0.2768 - val_acc: 0.0140\n",
      "Epoch 10/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2864 - acc: 0.0146 - val_loss: 0.2765 - val_acc: 0.0140\n",
      "Epoch 11/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2862 - acc: 0.0146 - val_loss: 0.2765 - val_acc: 0.0140\n",
      "Epoch 12/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2862 - acc: 0.0146 - val_loss: 0.2766 - val_acc: 0.0140\n",
      "Epoch 13/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2862 - acc: 0.0147 - val_loss: 0.2763 - val_acc: 0.0140\n",
      "Epoch 14/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2862 - acc: 0.0146 - val_loss: 0.2765 - val_acc: 0.0140\n",
      "Epoch 15/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2862 - acc: 0.0146 - val_loss: 0.2764 - val_acc: 0.0140\n",
      "Epoch 16/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2861 - acc: 0.0147 - val_loss: 0.2765 - val_acc: 0.0140\n",
      "Epoch 17/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2860 - acc: 0.0147 - val_loss: 0.2765 - val_acc: 0.0140\n",
      "Epoch 18/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2861 - acc: 0.0147 - val_loss: 0.2765 - val_acc: 0.0140\n",
      "Epoch 19/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2861 - acc: 0.0147 - val_loss: 0.2761 - val_acc: 0.0140\n",
      "Epoch 20/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2855 - acc: 0.0147 - val_loss: 0.2751 - val_acc: 0.0141\n",
      "Epoch 21/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2832 - acc: 0.0148 - val_loss: 0.2727 - val_acc: 0.0145\n",
      "Epoch 22/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2810 - acc: 0.0154 - val_loss: 0.2713 - val_acc: 0.0145\n",
      "Epoch 23/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2796 - acc: 0.0154 - val_loss: 0.2708 - val_acc: 0.0148\n",
      "Epoch 24/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2785 - acc: 0.0155 - val_loss: 0.2699 - val_acc: 0.0148\n",
      "Epoch 25/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2773 - acc: 0.0157 - val_loss: 0.2692 - val_acc: 0.0150\n",
      "Epoch 26/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2759 - acc: 0.0161 - val_loss: 0.2664 - val_acc: 0.0156\n",
      "Epoch 27/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2736 - acc: 0.0166 - val_loss: 0.2648 - val_acc: 0.0161\n",
      "Epoch 28/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2721 - acc: 0.0168 - val_loss: 0.2639 - val_acc: 0.0167\n",
      "Epoch 29/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2704 - acc: 0.0175 - val_loss: 0.2620 - val_acc: 0.0169\n",
      "Epoch 30/30\n",
      "3131/3131 [==============================] - 13s 4ms/step - loss: 0.2717 - acc: 0.0173 - val_loss: 0.2630 - val_acc: 0.0167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b2ada3a20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, batch_size=64, epochs=30, validation_split=.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
